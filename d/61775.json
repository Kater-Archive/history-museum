{"cacheTime": 1595549740, "baseUrl": "https://kater.me/d/61775", "title": "Google 跟柏克萊寫出全新讓靜態圖 2D 轉 3D 的無痛方式！", "content": [{"author": "天涯之外", "body": "\n<p>\n ＊原文網址：\n <a href=\"https://buzzorange.com/techorange/2020/04/27/google-berkeley-360-3d-photos/\" rel=\"nofollow noreferrer\" target=\"_blank\">\n https://buzzorange.com/techorange/2020/04/27/google-berkeley-360-3d-photos/\n </a>\n</p>\n<hr/>\n<p>\n 看到這張恐龍化石的動態圖片，你肯定會認為是用影片截出來的吧？\n <br/>\n<a href=\"https://storage.googleapis.com/nerf_data/website_renders/trike_200k_rgb.mp4?_=11\" rel=\"nofollow noreferrer\" target=\"_blank\">\n https://storage.googleapis.com/nerf_data/website_renders/trike_200k_rgb.mp4?_=11\n </a>\n</p>\n<hr/>\n<p>\n 然而真相卻是——完全由\n <strong>\n 靜態圖片\n </strong>\n 生成！\n </p>\n<p>\n 沒錯，而且還是\n <strong>\n 不用 3D 建模\n </strong>\n 的那種。\n </p>\n<p>\n 這就是來自\n <strong>\n 柏克萊大學\n </strong>\n 和\n <strong>\n Google\n </strong>\n 的最新研究：\n <strong>\n NeRF\n </strong>\n ，只需要輸入 少量靜態圖片 ，就能做到多視角的 逼真 3D 效果 。\n <br/>\n ￼\n <br/>\n 還需要專門說明的是，這項研究的程式碼和數據，也都已經\n <strong>\n<a href=\"https://github.com/bmild/nerf\" rel=\"nofollow noreferrer\" target=\"_blank\">\n 開源\n </a>\n</strong>\n 。你有想法，盡情一試。\n </p>\n<hr/>\n<blockquote class=\"uncited\">\n<div>\n<p>\n 這篇文章的影片不是放在常見平臺，卡特沒法預覽，所以要看更多細節請到上方原文網址。\n <br/>\n 如果我有 GPU 的話，真的好想來玩玩看…\n </p>\n<h3>\n 附上幾張動圖（來自 bmild/nerf Readme.md）\n </h3>\n<p>\n<img alt=\"\" src=\"https://camo.githubusercontent.com/a1db0785b874053ec0d3145a08f0476a4395f3e9/68747470733a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e626d696c642f6e6572662f6665726e5f3230306b5f323536772e676966\" title=\"\"/>\n<br/>\n<img alt=\"\" src=\"https://camo.githubusercontent.com/e1eb81fbee2a020564a28c816f18c00a21930d2d/68747470733a2f2f70656f706c652e656563732e6265726b656c65792e6564752f7e626d696c642f6e6572662f6c65676f5f3230306b5f323536772e676966\" title=\"\"/>\n<br/>\n .\n </p>\n<h3>\n 官方影片簡介\n </h3>\n<p>\n<span data-s9e-mediaembed=\"youtube\" style=\"display:inline-block;width:100%;max-width:640px\">\n<span style=\"display:block;overflow:hidden;position:relative;padding-bottom:56.25%\">\n<iframe allowfullscreen=\"\" scrolling=\"no\" src=\"https://www.youtube.com/embed/JuH79E8rdKc\" style=\"background:url(https://i.ytimg.com/vi/JuH79E8rdKc/hqdefault.jpg) 50% 50% / cover;border:0;height:100%;left:0;position:absolute;width:100%\">\n</iframe>\n</span>\n</span>\n</p>\n</div>\n</blockquote>\n"}, {"author": "姆蜜一雙藍黑狂暴巨雙炫劍擬招重砍一夏天仲玉模式", "body": "\n<p>\n 越來越邱囉 色圖可以模擬360度出來嗎&gt;&lt;\n </p>\n"}, {"author": "CCCㅤ", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682043\" href=\"https://kater.me/d/61775/3\">\n 姆蜜一雙藍黑狂暴巨雙炫劍擬招重砍一夏天仲玉模式\n </a>\n<br/>\n 假如你說的是日漫風格的色圖\n <br/>\n 漫畫對於人體結構透視在不同角度下有很大程度的（故意的）變形，沒有一個單一模型可以在三維空間中表示他\n <br/>\n 對輸入的假設就不對的話，會產生什麼奇怪的輸出應該難以預料…XD\n </p>\n"}, {"author": "別再打了我不會認錯的", "body": "\n<p>\n 幹有夠屌\n </p>\n"}, {"author": "AntesDeIrteADormir", "body": "\n<p>\n 被錢限制的運算能力\n </p>\n"}, {"author": "天涯之外", "body": "\n<p>\n 是說製作一個幾秒的 3D 影片就要動輒二十萬次迭代…\n </p>\n<p>\n 看來… 凡人還是看看就好，玩不起玩不起…\n </p>\n"}, {"author": "CCCㅤ", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682053\" href=\"https://kater.me/d/61775/6\">\n 天涯之外\n </a>\n 那個是應該是指 training 的時間, 單純使用一個 train 好的神經網路是很快的\n </p>\n"}, {"author": "Fruit丨vinegar", "body": "\n<p>\n 之前好想有類似\n </p>\n<p>\n 晃奶子的圖\n </p>\n"}, {"author": "天涯之外", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682061\" href=\"https://kater.me/d/61775/7\">\n Fruit丨vinegar\n </a>\n 現在你可以自己做了。如果你想且你有本錢的話….\n </p>\n"}, {"author": "桃園阿澤", "body": "\n<p>\n Live 3D要出來了嗎？\n </p>\n"}, {"author": "天涯之外", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682262\" href=\"https://kater.me/d/61775/11\">\n CCCㅤ\n </a>\n 是這樣嗎？\n </p>\n<p>\n 那爲什麼他描述這樣\n </p>\n<pre data-hljs=\"\"><code class=\"language-text\">After 200k iterations (about 15 hours), \nyou should get a video like this at logs/fern_test/fern_test_spiral_200000_rgb.mp4</code></pre>\n<script>\n if(\"undefined\"!==typeof hljs)hljs._ha();else if(\"undefined\"===typeof hljsLoading){hljsLoading=1;var a=document.getElementsByTagName(\"head\")[0],e=document.createElement(\"link\");e.type=\"text/css\";e.rel=\"stylesheet\";e.href=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css\";a.appendChild(e);e=document.createElement(\"script\");e.type=\"text/javascript\";e.onload=function(){var d={},f=0;hljs._hb=function(b){b.removeAttribute(\"data-hljs\");var c=b.innerHTML;c in d?b.innerHTML=d[c]:(7<++f&&(d={},f=0),hljs.highlightBlock(b.firstChild),d[c]=b.innerHTML)};hljs._ha=function(){for(var b=document.querySelectorAll(\"pre[data-hljs]\"),c=b.length;0<c;)hljs._hb(b.item(--c))};hljs._ha()};e.async=!0;e.src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/highlight.min.js\";a.appendChild(e)}\n </script>\n<p>\n 看起來就好像「經過二十萬次迭代可以得到那個影片」\n <br/>\n 而且它的檔名也直接把迭代次數加進去命名，所以我才會這樣想\n </p>\n"}, {"author": "CCCㅤ", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682267\" href=\"https://kater.me/d/61775/12\">\n 天涯之外\n </a>\n 那是叫你自己 train 完再 render, 他也有提供 pre-trained model\n <br/>\n 可以試試看\n <a href=\"https://github.com/bmild/nerf#rendering-a-nerf\" rel=\"nofollow noreferrer\" target=\"_blank\">\n https://github.com/bmild/nerf#rendering-a-nerf\n </a>\n</p>\n"}, {"author": "天涯之外", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682306\" href=\"https://kater.me/d/61775/13\">\n CCCㅤ\n </a>\n 大佬，我看懂了 😂\n </p>\n"}, {"author": "全都不認識-邊緣到真的全都不認識", "body": "\n<p>\n 怎麼還沒開放民眾用\n </p>\n"}, {"author": "天涯之外", "body": "\n<p>\n<a class=\"PostMention\" data-id=\"682346\" href=\"https://kater.me/d/61775/15\">\n 全都不認識-邊緣到真的全都不認識\n </a>\n 蛤\n </p>\n"}]}